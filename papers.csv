Paper,Authors,Year,Summary,Link
Attention Is All You Need,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser",2017,"Introduced the Transformer architecture, which relies entirely on self-attention mechanisms to draw global dependencies between input and output. It demonstrated superior performance in tasks like machine translation and has become the foundation for many subsequent models.",https://arxiv.org/pdf/1706.03762
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",2018,"Presented BERT (Bidirectional Encoder Representations from Transformers), a model pre-trained on a large corpus in a self-supervised manner. BERT achieved state-of-the-art performance on a variety of NLP tasks by leveraging bidirectional context.",https://arxiv.org/pdf/1810.04805
ULMFiT: Universal Language Model Fine-tuning for Text Classification,"Jeremy Howard, Sebastian Ruder",2018,"Proposed ULMFiT, a method for fine-tuning pre-trained language models for downstream tasks. This approach demonstrated that pre-trained language models could be effectively adapted to various NLP tasks with minimal task-specific training data.",https://arxiv.org/pdf/1801.06146
RoBERTa: A Robustly Optimized BERT Pretraining Approach,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",2019,Improved upon BERT by optimizing pre-training and increasing the amount of data used. RoBERTa demonstrated that more robust training techniques and larger datasets could significantly enhance model performance.,https://arxiv.org/pdf/1907.11692
XLNet: Generalized Autoregressive Pretraining for Language Understanding,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",2019,"Introduced XLNet, a model that combines the strengths of autoregressive and autoencoding models. It outperformed BERT on several benchmarks by capturing bidirectional context without relying on corrupting input data.",https://arxiv.org/pdf/1906.08237
T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",2020,"Proposed the T5 model, which frames all NLP tasks as a text-to-text problem, enabling a unified approach to training. This paradigm shift allowed for more straightforward model training and evaluation across diverse tasks.",https://arxiv.org/pdf/1910.10683
Electra: Pre-training Text Encoders as Discriminators Rather Than Generators,"Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",2020,Proposed a new pre-training method where a discriminator distinguishes real input tokens from replaced ones generated by a generator. Electra showed that this approach could be more compute-efficient and achieve better performance than masked language models like BERT.,https://arxiv.org/pdf/2003.10555
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut",2019,"Presented ALBERT, a lighter and faster variant of BERT that uses parameter sharing and factorized embeddings to reduce model size and training time. Despite its efficiency, ALBERT maintained competitive performance on various benchmarks.",https://arxiv.org/pdf/1909.11942
Deep Contextualized Word Representations,"Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer",2018,"Introduced ELMo (Embeddings from Language Models), which generates context-sensitive word representations using deep bidirectional LSTMs. This approach demonstrated the effectiveness of contextual embeddings in improving performance across various NLP tasks.",https://arxiv.org/pdf/1802.05365
Exploring the Limits of Language Modeling,"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu",2016,Investigated the performance limits of neural language models by experimenting with different architectures and training methods. This paper provided insights into optimizing model architectures and training techniques for better language modeling.,https://arxiv.org/pdf/1602.02410
The Evolved Transformer,"David R. So, Chen Liang, Quoc V. Le",2019,Introduced a variant of the Transformer architecture designed through neural architecture search. The Evolved Transformer demonstrated superior performance and efficiency compared to the original Transformer on various tasks.,https://arxiv.org/pdf/1901.11117
Language Models as Knowledge Bases?,"Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel",2019,Investigated the ability of pre-trained language models to function as knowledge bases by probing them for factual knowledge. This paper highlighted the potential of language models to store and retrieve world knowledge without explicit external databases.,https://arxiv.org/pdf/1909.01066
CTRL: A Conditional Transformer Language Model for Controllable Generation,"Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher",2019,"Presented the Conditional Transformer Language Model (CTRL), which allows users to control text generation through predefined control codes. This approach provided a way to steer the generation process towards specific styles or topics.",https://arxiv.org/pdf/1909.05858
Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,"William Fedus, Barret Zoph, Noam Shazeer",2021,"Proposed the Switch Transformer, which uses a mixture-of-experts approach to scale models to trillions of parameters efficiently. This method allowed for significant scaling without a proportional increase in computational cost.",https://arxiv.org/pdf/2101.03961
Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",2020,"Introduced the Retrieval-Augmented Generation (RAG) model, which combines retrieval and generation mechanisms to handle knowledge-intensive tasks. This approach improved performance on tasks requiring access to external knowledge.",https://arxiv.org/pdf/2005.11401
UnifiedQA: Crossing Format Boundaries with a Single QA System,"Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",2020,"Proposed UnifiedQA, a model designed to handle multiple question-answering formats using a unified approach. This work demonstrated the potential of a single model to perform well across diverse QA tasks by leveraging pre-training on a variety of formats.",https://arxiv.org/pdf/2005.00700
GPT-3: Language Models are Few-Shot Learners,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020,"Introduced GPT-3, a language model with 175 billion parameters that demonstrated the ability to perform tasks with few-shot, one-shot, or zero-shot learning. This paper highlighted the potential of scaling up models to achieve superior performance across diverse tasks.",https://arxiv.org/pdf/2005.14165